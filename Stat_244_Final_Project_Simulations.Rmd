---
title: "Stat 244 Final Project"
author: "Asher Spector"
date: "12/14/2020"
output: html_document
---

## Setup

```{r}
# Running this script also requires glmhd
# but we do not load it to prevent namespace errors.
library(tidyverse)
```

## Plotting alpha, sigma, lambda

In this section, we plot $\kappa, \gamma, h_{\mathrm{MLE}}$ against each other. The *glmhd* package contains functions which can solve such systems of equations involving intractable integrals (based on turn on the *cubature* package for low-dimensional integration).

```{r}
# Create grid
kappa_vals = c(0.05, 0.1, 0.15, 0.2)
max_gammas = c(15, 9.5, 5, 3) # Hard-coded hmles because it's hard to compute hmle in R
#kappa_vals = c(0.2)
#max_gammas = c(4)
gamma_vals = c(2, 4, 6, 8, 10)
output = tibble(
  kappa=numeric(),
  gamma=numeric(),
  alpha=numeric(),
  sigma=numeric(),
  lambda=numeric(),
)

# Iterate through and solve system of equations.
# This can be pretty slow!
for (index in 1:length(kappa_vals)) {
  kappa = kappa_vals[index]
  max_gamma = max_gammas[index]
  for (gamma in gamma_vals) {
    if (gamma < max_gamma) {
      # Pattern: alpha, lambda, sigma
      cat("At gamma=", gamma, "kappa=", kappa)
      params <- glmhd::find_param(
        kappa=kappa, 
        gamma=gamma, 
        intercept=FALSE
      )
      output <- output %>% add_row(
        kappa=kappa, 
        gamma=gamma,
        alpha=params[1],
        lambda=params[2],
        sigma=params[3]
      ) 
    }
  }
}

# write.table(output, "cache/param.csv")
## See the python notebook for the script plotting based on these values.

```

### Simulation Results

We begin by creating a general function for sampling covariates and a binary response and then fitting a logistic regression model. We let $X \sim \mathcal{N}(0, \Sigma)$ where $X$ follows a non-stationary Markov chain. In particular, we can represent $X_1 \sim \mathcal{N}(0, 1)$ and $X_j = \sqrt{1 - \rho_j^2}  Z_j + \rho_j X_{j-1}$ with $Z_j \stackrel{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0,1)$. We sample $\rho_j \stackrel{\mathrm{i.i.d.}}{\sim} \mathrm{Uniform}(0, 1)$.

```{r}

# Samples from gaussian Markov chain
sample_gaussian_markov <- function(n=500, p=75) {
  # Step 1: Create the covariance matrix
  log_rhos = c(0, log(runif(p-1)))
  cumrhos = cumsum(log_rhos)
  cumrhos = do.call(rbind, replicate(p, cumrhos, simplify=FALSE))
  log_corrs = -1*abs(t(cumrhos) - cumrhos)
  Sigma = exp(log_corrs)

  # Step 2: Sample the normal data
  X = MASS::mvrnorm(n=n, mu=replicate(p, 0), Sigma=Sigma)
  return(list(X=X, Sigma=Sigma))
}
# Parameters:
# n, p: number of data points and covariates
# xdist: either 'gausisan' for gaussian markov chain or 't' for heavy-tailed markov chain.
# ydist: binomial for logistic response, probit for probit response.
# signal: non-null covariates equal +/- signal / sqrt(p).
sample_data_and_fit <- function(n=500, p=100, signal=1, xdist='gaussian', ydist='binomial') {
  # Step 1: Sampling / data generating process
  # Create 50% sparse coefficients
  beta = replicate(p, 0)
  signs = 1-2*rbinom((p/2), 1, 0.5)
  beta[1:(p/2)] = signal / sqrt(p)
  beta[1:(p/2)] = beta[1:(p/2)] * signs
  # Sample X data
  if (xdist == 'gaussian') {
    X = sample_gaussian_markov(n=n, p=p)$X
  } else {
    #X = sample_t_markov(n=n, p=p)
  }
  # Sample y data
  mu = X %*% beta
  if (ydist == 'binomial') {
    y = rbinom(n, 1, 1/(1 + exp(-mu)))
  } else {
    y = mu + rnorm(p)
    y = y > 0
  }
  
  # Step 2: Fit regular logistic regression
  fit <- glm(y ~ X + 0, family=binomial, x=TRUE, y=TRUE)
  return(list(fit=fit, beta=beta))
}

# Produces summaries of p values, bias, etc for adjusted vs. regular
compare_adjusted_versus_regular <- function(nreps=5, ...) {

  # We parallelize the adjusted_pval calls on 10 cores  
  # (note this will NOT run on Windows since it uses forking
  # and I'm not sure if it will run on mac).
  partial_function <- function(seed) {
    set.seed(seed)
    out = sample_data_and_fit(...)
    glm_fit = out$fit
    beta = out$beta
    adjusted_fit = glmhd::adjust_glm(glm_fit)
    return(list(fit=glm_fit, beta=beta, adjusted_fit=adjusted_fit))
  }

  start_time = proc.time()[3]
  # Use this instead of mcapply fails
  #for (i in 1:nreps) {
  #  cat("At time=", proc.time()[3]-start_time, " starting seed=", i, "\n", sep='')
  #  all_fits[[i]] = partial_function(i)
  #}
  all_fits = parallel::mclapply(200:(200+nreps), partial_function, mc.cores=10)
  cat("Finished at time=", proc.time()[3]-start_time, sep='')
  return(all_fits)

}

# Random number generators
all_fits = compare_adjusted_versus_regular(nreps=10, signal=5, n=500, p=100)

```


```{r}

# Holds all of the data
all_data = list()

# Analyze the fits
for (i in 1:length(all_fits)) {
  # Retrieve
  glm_coefs = summary(all_fits[[i]]$fit)$coefficients
  p = dim(glm_coefs)[1]
  glm_coefs = data.frame(glm_coefs) %>%
    rename(std = Std..Error) %>%
    rename(p.value = Pr...z..)
  glm_coefs$beta = all_fits[[i]]$beta
  glm_coefs$variable = 1:p
  glm_coefs$adj = FALSE
  all_data[[(2*i)]] = glm_coefs
  adj_coefs = summary(all_fits[[i]]$adjusted_fit)$coefficients
  adj_coefs = data.frame(adj_coefs) %>%
    rename(Estimate = adjusted_mle)
  adj_coefs$beta = all_fits[[i]]$beta
  adj_coefs$variable = 1:p
  adj_coefs$adj = TRUE
  all_data[[(2*i + 1)]] = adj_coefs
}

all_df = dplyr::bind_rows(all_data) %>%
  mutate(null = beta == 0)
ftime = strsplit(as.character(proc.time()[3]), split='.', fixed=TRUE)[[1]][1]
fname = paste("cache/", ftime, "_results.csv", sep='')
write.table(all_df, fname, sep=',')
```

```{r}

pval_plot <- ggplot(all_df %>% filter(variable==37 | variable == 75), aes(x=p.value, color=adj, fill=adj)) +
   geom_histogram(aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]), bins=10) +
   facet_wrap("null~adj", scales='free') +
   labs(x='p', y='Proportion')
print(pval_plot)

# Coefficients
```

