---
title: "Stat 244 Final Project"
author: "Asher Spector"
date: "12/14/2020"
output: html_document
---

## Setup

```{r}
# Running this script also requires glmhd
# but we do not load it to prevent namespace errors.
library(tidyverse)
```

## Plotting alpha, sigma, lambda

In this section, we plot $\kappa, \gamma, h_{\mathrm{MLE}}$ against each other. The *glmhd* package contains functions which can solve such systems of equations involving intractable integrals (based on turn on the *cubature* package for low-dimensional integration).

```{r}
# Create grid
kappa_vals = c(0.05, 0.1, 0.15, 0.2)
max_gammas = c(15, 9.5, 5, 3) # Hard-coded hmles because it's hard to compute hmle in R
#kappa_vals = c(0.2)
#max_gammas = c(4)
gamma_vals = c(2, 4, 6, 8, 10)
output = tibble(
  kappa=numeric(),
  gamma=numeric(),
  alpha=numeric(),
  sigma=numeric(),
  lambda=numeric(),
)

# Iterate through and solve system of equations.
# This can be pretty slow!
for (index in 1:length(kappa_vals)) {
  kappa = kappa_vals[index]
  max_gamma = max_gammas[index]
  for (gamma in gamma_vals) {
    if (gamma < max_gamma) {
      # Pattern: alpha, lambda, sigma
      cat("At gamma=", gamma, "kappa=", kappa)
      params <- glmhd::find_param(
        kappa=kappa, 
        gamma=gamma, 
        intercept=FALSE
      )
      output <- output %>% add_row(
        kappa=kappa, 
        gamma=gamma,
        alpha=params[1],
        lambda=params[2],
        sigma=params[3]
      ) 
    }
  }
}

# write.table(output, "cache/param.csv")
## See the python notebook for the script plotting based on these values.

```

### Simulation Results

We begin by creating a general function for sampling covariates and a binary response and then fitting a logistic regression model. We let $X \sim \mathcal{N}(0, \Sigma)$ where $X$ follows a non-stationary Markov chain. In particular, we can represent $X_1 \sim \mathcal{N}(0, 1)$ and $X_j = \sqrt{1 - \rho_j^2}  Z_j + \rho_j X_{j-1}$ with $Z_j \stackrel{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0,1)$. We sample $\rho_j \stackrel{\mathrm{i.i.d.}}{\sim} \mathrm{Uniform}(0, 1)$.


```{r}

# Samples from gaussian Markov chain
sample_gaussian_markov <- function(n=500, p=75) {
  # Step 1: Create the covariance matrix
  log_rhos = c(0, log(runif(p-1)))
  cumrhos = cumsum(log_rhos)
  cumrhos = do.call(rbind, replicate(p, cumrhos, simplify=FALSE))
  log_corrs = -1*abs(t(cumrhos) - cumrhos)
  Sigma = exp(log_corrs)

  # Step 2: Sample the normal data
  X = MASS::mvrnorm(n=n, mu=replicate(p, 0), Sigma=Sigma)
  return(X)
}

# Sample heavy-tailed Markov chain
sample_t_markov <- function(n=500, p=75) {
  # Sample rhos
  rhos = runif(p-1)
  # Initial values
  X = matrix(0, n, p)
  X[,1] = sqrt(1/3) * rt(n, df=3)
  # Loop through
  for (j in 2:(p)) {
    rho = rhos[j-1]
    X[,j] = sqrt(1-rho^2) * sqrt(1/3) * rt(n, df=3) + rho*X[,j-1]
  }
  return(X)
}

# Parameters:
# n, p: number of data points and covariates
# xdist: either 'gausisan' for gaussian markov chain or 't' for heavy-tailed markov chain.
# ydist: binomial for logistic response, probit for probit response.
# signal: non-null covariates equal +/- signal / sqrt(p).
sample_data_and_fit <- function(n=500, p=100, signal=1, xdist='gaussian', ydist='binomial') {
  # Step 1: Sampling / data generating process
  # Create 50% sparse coefficients
  beta = replicate(p, 0)
  signs = 1-2*rbinom((p/2), 1, 0.5)
  beta[1:(p/2)] = signal / sqrt(p)
  beta[1:(p/2)] = beta[1:(p/2)] * signs
  # Sample X data
  if (xdist == 'gaussian') {
    X = sample_gaussian_markov(n=n, p=p)
  } else {
    X = sample_t_markov(n=n, p=p)
  }
  # Sample y data
  mu = X %*% beta
  if (ydist == 'binomial') {
    y = rbinom(n, 1, 1/(1 + exp(-mu)))
  } else {
    y = mu + rnorm(n)
    y = y > 0
  }
  
  # Step 2: Fit regular logistic regression
  fit <- suppressWarnings(glm(y ~ X + 0, family=binomial, x=TRUE, y=TRUE))
  return(list(fit=fit, beta=beta))
}
```

The next two functions allow us to run these simulations many times and compare the classical and adjusted results.

```{r}
# Likelihood-ratio test p-values. These coords are all null.
LRT_coords = c(38, 42, 47, 52, 56, 51, 65, 70, 75)
compute_lrt_pvals <- function(glm_fit, adjusted_fit) {
  # We compute LRT p-vals for coordinates 38, 47, 56, 65, 74
  # to reduce computational expense.
  y = glm_fit$y
  X = glm_fit$x
  lrt_pvals = c()
  adj_lrt_pvals = c()
  for (i in 1:length(LRT_coords)) {
    index = LRT_coords[i]
    inner_fit = glm(y ~ X[, -index] + 0, family=binomial, x=TRUE, y=TRUE)
    classic_lrt <- lmtest::lrtest(glm_fit, inner_fit)
    lrt_fit <- glmhd::lrt_glm(list(glm_fit, inner_fit), param=adjusted_fit$param)
    lrt_pvals = c(lrt_pvals, classic_lrt$`Pr(>Chisq)`[2])
    adj_lrt_pvals = c(adj_lrt_pvals, lrt_fit$anova.tab$p.value[2])
  }
  print(adj_lrt_pvals)
  return(data.frame(
    classical=lrt_pvals,
    adjusted=adj_lrt_pvals,
    variable=LRT_coords
  ))
}

# Produces summaries of p values, bias, etc for adjusted vs. regular
compare_adjusted_versus_regular <- function(nreps=5, start=1, ...) {

  # We parallelize the adjusted_pval calls on 10 cores  
  # (note this will NOT run on Windows since it uses forking
  # and I'm not sure if it will run on mac).
  partial_function <- function(seed) {
    set.seed(seed)
    out = sample_data_and_fit(...)
    glm_fit = out$fit
    beta = out$beta
    adjusted_fit = glmhd::adjust_glm(glm_fit)
    lrt_pvals = compute_lrt_pvals(glm_fit, adjusted_fit)
    return(list(fit=glm_fit, beta=beta, adjusted_fit=adjusted_fit, lrt_pvals=lrt_pvals))
  }

  start_time = proc.time()[3]
  # This is faster
  all_fits = parallel::mclapply(start:(nreps+start-1), partial_function, mc.cores=10)
  cat("Finished at time=", proc.time()[3]-start_time, sep='')
  return(all_fits)

}
```

Finally, we use this function to save the data so that we don't lose it if RStudio crashses. (These scripts are computationally inteisve, so caching is important!).

```{r}

# Bind the data together and transfer over to python for plotting
save_data <- function(all_fits) {
  # Initialize
  all_data = list()
  all_lrt = list()
  
  # Analyze the fits
  for (i in 1:length(all_fits)) {
    # Retrieve
    glm_coefs = summary(all_fits[[i]]$fit)$coefficients
    p = dim(glm_coefs)[1]
    glm_coefs = data.frame(glm_coefs) %>%
      rename(std = Std..Error) %>%
      rename(p.value = Pr...z..)
    glm_coefs$beta = all_fits[[i]]$beta
    glm_coefs$variable = 1:p
    glm_coefs$adj = FALSE
    all_data[[(2*i)]] = glm_coefs
    adj_coefs = summary(all_fits[[i]]$adjusted_fit)$coefficients
    adj_coefs = data.frame(adj_coefs) %>%
     rename(Estimate = adjusted_mle)
    adj_coefs$beta = all_fits[[i]]$beta
    adj_coefs$variable = 1:p
    adj_coefs$adj = TRUE
    all_data[[(2*i + 1)]] = adj_coefs
    
    # LRT pvals
    all_lrt[[i]] = all_fits[[i]]$lrt_pvals
  }
  
  all_df = dplyr::bind_rows(all_data) %>%
    mutate(null = beta == 0)
  lrt_df = dplyr::bind_rows(all_lrt)
  ftime = strsplit(as.character(proc.time()[3]), split='.', fixed=TRUE)[[1]][1]
  fname = paste("cache/", ftime, "_results.csv", sep='')
  lrtname = paste("cache/", ftime, "_results_lrt.csv", sep='')
  write.table(all_df, fname, sep=',')
  write.table(lrt_df, lrtname, sep=',')
}
```

### Simulation Setting 1

```{r}

# Cache occasionally
for (j in 1:10) {
  cat("At j=", j)
  all_fits = compare_adjusted_versus_regular(start=j*100, nreps=40, signal=3, n=500, p=75)
  save_data(all_fits)
}
```

### Simulation Setting 2

```{r}

# Cache occasionally
for (j in 1:10) {
  cat("At j=", j)
  all_fits = compare_adjusted_versus_regular(start=j*100, nreps=40, signal=2, n=500, p=75, xdist='t', ydist='probit')
  save_data(all_fits)
}
```

